## This piece of code came about when I was doing research on how to create
## a web crawler for my computer science project. I found a website with
## many articles and I wanted to read them all, but I didn't want to download
## the articles manually, so I created the following program to extract all
## the article contents from the links.


import html.parser
import urllib.request

from urllib.request import urlopen

# Open the webpage with all the juicy links here...
website = "(input home page link here: http://...)"
page = urlopen(website)

import bs4
from bs4 import BeautifulSoup

soup = BeautifulSoup(page,"html.parser")

soup.prettify()

# The following extracts all links from the given home page.
website_line = []
for link in soup.find_all('a'):
    website_line.append(link.get('href'))

# The following visits each link stored in the array and scrapes the contents!
length = len(website_line)
for i in range(1,length):
    page = urlopen(website_line[i])
    soup = BeautifulSoup(page,"html.parser")
    soup.prettify()
    ## The following 6 lines collects headers of <h1> format, all paragraphs and all ordered lists.
    ## They may be modified to scrape, say, images <img> and other tags.
    for txt in soup.find_all('h1'):
        print(txt)
    for txt in soup.find_all('p'):
        print(txt)
    for txt in soup.find_all('ol'):
        print(txt)
    ## print(i) ## optional line to keep track of the number of websites visited; also helpful when debugging
    print('++++++++++++++++++++++')
